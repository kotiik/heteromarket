import importlib
import unittest
import torch


_core = importlib.import_module("heteromarket.core")
find_equilibrium_prices = _core.find_equilibrium_prices


class TestDirectionalGradientsExample(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        torch.set_default_dtype(torch.float64)

    def setUp(self):
        # Problem with broken symmetry (deterministic)
        self.B, self.N = 2, 2

        # Fixed inputs (require grads for the targets we differentiate)
        self.Sigma = torch.tensor(
            [
                [[0.7086, 0.2066], [0.2066, 0.5898]],
                [[0.4318, 0.6247], [0.6247, 2.6645]],
            ],
            requires_grad=True,
            dtype=torch.float64,
        )
        self.mu = torch.tensor(
            [[2.0486, 2.3718], [2.0554, 2.3971]],
            requires_grad=True,
            dtype=torch.float64,
        )
        self.com = torch.zeros(self.B, dtype=torch.float64).requires_grad_(True)
        self.X = torch.zeros((self.B, self.N), dtype=torch.float64).requires_grad_(True)
        self.budget = torch.tensor([1.0, 1.3], dtype=torch.float64, requires_grad=True)
        self.kappa  = torch.tensor([0.15, 0.05], dtype=torch.float64, requires_grad=True)  # short caps
        self.theta  = torch.tensor([1.2,  1.6 ], dtype=torch.float64, requires_grad=True)  # long caps
        self.S      = torch.linspace(0.7, 1.3, self.N, dtype=torch.float64).requires_grad_(True)

        # Starting guess for prices
        self.p0 = torch.ones(self.N, dtype=torch.float64)

        # Fixed probing vector for the scalar functional <p_eq, v>
        self.v = torch.tensor([0.3428, -1.0417], dtype=torch.float64)

        # Small FD step
        self.eps = 3e-8

        # Relative and absolute tolerances
        self.rel_tol = 3e-4
        self.abs_tol = 5e-7  # used when true derivative magnitude is tiny

    # ---------- helpers ----------
    def _eq_prices(self, Sigma, mu, com, X, budget, kappa, theta, S, p0):
        return find_equilibrium_prices(Sigma, mu, com, X, budget, kappa, theta, S, initial_approximation=p0)

    def _scalar_loss(self, p):
        # Loss = <p, v>
        return (p * self.v).sum()

    @staticmethod
    def _fd_dir(fn, x, dx, eps):
        return (fn(x + eps * dx) - fn(x - eps * dx)) / (2.0 * eps)

    # ---------- tests ----------
    def test_budget_directional_derivative(self):
        # Forward
        p_eq = self._eq_prices(self.Sigma, self.mu, self.com, self.X,
                               self.budget, self.kappa, self.theta, self.S, self.p0)
        loss = self._scalar_loss(p_eq)

        # AD gradient wrt budget
        (g_budget,) = torch.autograd.grad(loss, (self.budget,), allow_unused=False)

        # FD directional derivative wrt budget
        Db = torch.tensor([-0.3, 0.2], dtype=torch.float64)

        def f_budget(bud):
            p = self._eq_prices(self.Sigma, self.mu, self.com, self.X, bud,
                                self.kappa, self.theta, self.S, self.p0)
            return self._scalar_loss(p)

        fd_dir = self._fd_dir(f_budget, self.budget, Db, self.eps)
        ad_dir = (g_budget * Db).sum()

        # Compare with relative error; fall back to abs if FD ~ 0
        if fd_dir.abs() > 1e-8:
            rel = (ad_dir - fd_dir).abs() / fd_dir.abs()
            self.assertLess(float(rel), self.rel_tol)
        else:
            self.assertLess(float((ad_dir - fd_dir).abs()), self.abs_tol)

    def test_kappa_directional_derivative(self):
        p_eq = self._eq_prices(self.Sigma, self.mu, self.com, self.X,
                               self.budget, self.kappa, self.theta, self.S, self.p0)
        loss = self._scalar_loss(p_eq)

        (g_kappa,) = torch.autograd.grad(loss, (self.kappa,), allow_unused=False)

        Dk = torch.tensor([0.25, -0.15], dtype=torch.float64)

        def f_kappa(ka):
            p = self._eq_prices(self.Sigma, self.mu, self.com, self.X, self.budget,
                                ka, self.theta, self.S, self.p0)
            return self._scalar_loss(p)

        fd_dir = self._fd_dir(f_kappa, self.kappa, Dk, self.eps)
        ad_dir = (g_kappa * Dk).sum()

        if fd_dir.abs() > 1e-8:
            rel = (ad_dir - fd_dir).abs() / fd_dir.abs()
            self.assertLess(float(rel), self.rel_tol)
        else:
            self.assertLess(float((ad_dir - fd_dir).abs()), self.abs_tol)

    def test_mu_directional_derivative(self):
        p_eq = self._eq_prices(self.Sigma, self.mu, self.com, self.X,
                               self.budget, self.kappa, self.theta, self.S, self.p0)
        loss = self._scalar_loss(p_eq)

        (g_mu,) = torch.autograd.grad(loss, (self.mu,), allow_unused=False)

        g = torch.Generator().manual_seed(123)
        Dmu = torch.randn(self.mu.shape, generator=g, dtype=torch.float64)

        def f_mu(mu_in):
            p = self._eq_prices(self.Sigma, mu_in, self.com, self.X, self.budget,
                                self.kappa, self.theta, self.S, self.p0)
            return self._scalar_loss(p)

        fd_dir = self._fd_dir(f_mu, self.mu, Dmu, self.eps)
        ad_dir = (g_mu * Dmu).sum()

        if fd_dir.abs() > 1e-8:
            rel = (ad_dir - fd_dir).abs() / fd_dir.abs()
            self.assertLess(float(rel), self.rel_tol)
        else:
            self.assertLess(float((ad_dir - fd_dir).abs()), self.abs_tol)

    def test_sigma_directional_derivative_pd_preserving(self):
        p_eq = self._eq_prices(self.Sigma, self.mu, self.com, self.X,
                               self.budget, self.kappa, self.theta, self.S, self.p0)
        loss = self._scalar_loss(p_eq)

        (g_Sigma,) = torch.autograd.grad(loss, (self.Sigma,), allow_unused=False)

        # Symmetric direction; use torch.randn(shape, generator=...)
        gS = torch.Generator().manual_seed(456)
        R = torch.randn(self.Sigma.shape, generator=gS, dtype=torch.float64)
        DS = 0.5 * (R + R.transpose(-1, -2))  # symmetric

        # Keep QP well-posed: add tiny ridge to both Â± evaluations
        ridge = 1e-6 * torch.eye(self.N, dtype=torch.float64).expand(self.B, self.N, self.N)

        def f_Sigma(Sig_in):
            p = self._eq_prices(Sig_in + ridge, self.mu, self.com, self.X, self.budget,
                                self.kappa, self.theta, self.S, self.p0)
            return self._scalar_loss(p)

        fd_dir = self._fd_dir(f_Sigma, self.Sigma, DS, self.eps)
        ad_dir = (g_Sigma * DS).sum()

        if fd_dir.abs() > 1e-8:
            rel = (ad_dir - fd_dir).abs() / fd_dir.abs()
            self.assertLess(float(rel), self.rel_tol)
        else:
            self.assertLess(float((ad_dir - fd_dir).abs()), self.abs_tol)


if __name__ == "__main__":
    unittest.main()
